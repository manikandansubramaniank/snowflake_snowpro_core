# 9. Production Best Practices

## Overview
Production dbt deployments require state management (artifacts), selective runs (--select/--exclude), cost optimization (incremental strategies, clustering), and monitoring (logs, query tags, alerts). These patterns ensure reliability, performance, and cost control at scale.

---

## 9.1 State Management & Artifacts

### What are Artifacts?
Artifacts are JSON metadata files generated by dbt (manifest.json, run_results.json, catalog.json). They enable state comparison for selective runs, deployment validation, and lineage tracking.

**Diagram: Artifact-Based State Comparison**
```text
Production Artifacts (S3/GCS)
     â”œâ”€â”€ manifest.json (model definitions, dependencies)
     â”œâ”€â”€ run_results.json (execution status, timings)
     â””â”€â”€ catalog.json (column-level metadata)
     â†“ dbt ls --select state:modified+ --state ./prod_artifacts
CI/CD (GitHub Actions)
     â†“ compare dev branch vs prod state
Modified Models Only (slim CI)
     â†“ dbt build --select state:modified+ --defer
Cost-Efficient Deployment (5 models vs 500)
```

### Key Artifacts

| Artifact | Purpose | Contains |
|----------|---------|----------|
| manifest.json | Compiled project graph | Models, tests, sources, dependencies, configs |
| run_results.json | Execution results | Success/failure status, execution time, row counts |
| catalog.json | Database metadata | Columns, types, statistics (from dbt docs generate) |
| sources.json | Source freshness | Freshness check results (from dbt source freshness) |

### Syntax: State-Based Selectors

**SYNTAX TEMPLATE**
```bash
dbt <command> --select <selector> --state <path_to_artifacts> [--defer]

# State selectors
state:modified       # Models with changed code
state:modified+      # Modified models + downstream
+state:modified      # Modified models + upstream
state:new            # Newly added models
result:<status>      # Models with specific run status (error, fail, success)
```

**PARAMETERS**

| Parameter | Type | Description | Example |
|-----------|------|-------------|---------|
| --state | Path | Directory containing production artifacts | ./prod_artifacts, s3://bucket/artifacts |
| --defer | Flag | Use production models for unbuilt upstream refs | --defer (no value required) |
| state:modified | Selector | Models with code changes vs state | state:modified |
| state:modified+ | Selector | Modified + all downstream models | state:modified+ |
| +state:modified | Selector | Modified + all upstream models | +state:modified |
| state:new | Selector | Models not in state artifacts | state:new |
| result:error | Selector | Models that errored in last run | result:error --state ./artifacts |
| result:fail | Selector | Models with test failures | result:fail |

**BASIC EXAMPLE: Slim CI**
```bash
# Download production artifacts from S3
aws s3 cp s3://dbt-artifacts/prod/manifest.json ./prod_artifacts/

# Run only modified models
dbt build --select state:modified+ --state ./prod_artifacts --defer --target dev

# Explanation:
# - state:modified+ : Modified models + downstream dependents
# - --defer : Use prod tables for unchanged upstream models (avoid rebuilding)
# - --target dev : Run in dev environment
```

**ADVANCED EXAMPLE: GitHub Actions Slim CI**
```yaml
# .github/workflows/dbt_slim_ci.yml
name: dbt Slim CI

on:
  pull_request:
    branches: [main]

jobs:
  slim_ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dbt
        run: pip install dbt-snowflake==1.7.0
      
      - name: Download production artifacts
        run: |
          mkdir -p prod_artifacts
          aws s3 cp s3://dbt-artifacts/prod/manifest.json prod_artifacts/
          aws s3 cp s3://dbt-artifacts/prod/run_results.json prod_artifacts/
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      
      - name: dbt deps
        run: dbt deps
      
      - name: dbt build (modified models only)
        run: |
          dbt build \
            --select state:modified+ \
            --state prod_artifacts \
            --defer \
            --target ci \
            --profiles-dir .
        env:
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
      
      - name: Comment PR with results
        if: always()
        run: |
          dbt run-operation generate_ci_report
          gh pr comment ${{ github.event.pull_request.number }} --body-file ci_report.md
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

**SDLC Use Case: Artifact Storage Strategy**
```bash
# Production deployment: upload artifacts after successful run
dbt build --target prod
dbt docs generate

# Upload to S3 with versioning
aws s3 cp target/manifest.json s3://dbt-artifacts/prod/manifest.json
aws s3 cp target/manifest.json s3://dbt-artifacts/versioned/manifest_$(date +%Y%m%d_%H%M%S).json
aws s3 cp target/run_results.json s3://dbt-artifacts/prod/run_results.json
aws s3 cp target/catalog.json s3://dbt-artifacts/prod/catalog.json

# Retention policy: keep 30 days of versioned artifacts
aws s3 ls s3://dbt-artifacts/versioned/ | \
  grep manifest_ | \
  awk '{print $4}' | \
  sort -r | \
  tail -n +31 | \
  xargs -I {} aws s3 rm s3://dbt-artifacts/versioned/{}
```

---

## 9.2 Selective Runs

### What are Selective Runs?
Selective runs execute subsets of models using --select and --exclude flags. Critical for CI/CD speed, cost control, and targeted debugging.

**Diagram: Selector Logic Flow**
```text
dbt ls --select tag:daily,config.materialized:incremental
     â†“ selector evaluation
Graph Traversal (parse manifest.json dependencies)
     â†“ union/intersection of selectors
Matched Models: [fct_orders, fct_events, dim_customers]
     â†“ dbt run --select <matched_models>
Execute Only Matched Models (skip 95% of project)
```

### Syntax: Selection & Exclusion

**SYNTAX TEMPLATE**
```bash
dbt <command> --select <selector1> <selector2> ... [--exclude <selector3>]

# Selectors:
<model_name>                    # Specific model
tag:<tag_name>                  # Models with tag
config.<config_key>:<value>     # Models with config value
path:<directory>                # Models in directory
package:<package_name>          # Models from package
source:<source_name>            # Source dependencies
exposure:<exposure_name>        # Exposure dependencies
resource_type:<type>            # Type filter (model, test, seed, snapshot)
state:modified                  # State-based selection

# Graph operators:
+<selector>                     # Upstream dependencies
<selector>+                     # Downstream dependencies
<selector>+n                    # n levels downstream
@<selector>                     # Model + children (1 level)
```

**PARAMETERS**

| Selector | Type | Description | Example |
|----------|------|-------------|---------|
| model_name | String | Specific model by name | fct_orders, stg_customers |
| tag:name | Tag | Models with specific tag | tag:daily, tag:pii |
| config.key:value | Config | Models with config setting | config.materialized:incremental |
| path:directory | Path | Models in directory (wildcards ok) | path:models/marts/sales |
| source:name | Source | Models depending on source | source:raw_data.customers |
| exposure:name | Exposure | Models used by exposure | exposure:sales_dashboard |
| state:modified | State | Changed models (requires --state) | state:modified+ |
| +selector | Graph | Upstream parents | +fct_orders (includes staging) |
| selector+ | Graph | Downstream children | stg_orders+ (includes all marts) |
| selector+n | Graph | n-levels downstream | stg_orders+2 (2 levels) |
| @selector | Graph | Model + direct children only | @stg_orders |

**BASIC EXAMPLES**
```bash
# Run specific model
dbt run --select fct_orders

# Run model + upstream dependencies
dbt run --select +fct_orders

# Run model + downstream dependencies
dbt run --select fct_orders+

# Run model + upstream + downstream (full lineage)
dbt run --select +fct_orders+

# Run all models with tag
dbt run --select tag:daily

# Run incremental models only
dbt run --select config.materialized:incremental

# Run models in directory
dbt run --select path:models/marts/sales

# Exclude specific models
dbt run --select tag:hourly --exclude fct_events

# Union selectors (OR logic)
dbt run --select tag:daily tag:hourly

# Intersection (use separate --select flags)
dbt run --select tag:daily --select config.materialized:incremental
```

**ADVANCED EXAMPLES**
```bash
# Modified models + downstream, exclude snapshots
dbt build \
  --select state:modified+ \
  --exclude resource_type:snapshot \
  --state ./prod_artifacts \
  --defer

# Models depending on specific source
dbt run --select source:raw_data.orders+

# Models used by critical exposures
dbt run --select +exposure:executive_dashboard +exposure:ml_churn_model

# Re-run failed models from last run
dbt run --select result:error --state ./artifacts

# Run staging + marts, exclude intermediate
dbt run --select path:models/staging path:models/marts --exclude path:models/intermediate

# Run 2 levels downstream from modified models
dbt run --select state:modified+2 --state ./prod_artifacts

# Complex: daily incremental models, excluding PII-tagged, with tests
dbt build \
  --select tag:daily,config.materialized:incremental \
  --exclude tag:pii \
  --state ./prod_artifacts
```

**Selector YAML (dbt v1.5+)**
```yaml
# selectors.yml
selectors:
  - name: daily_refresh
    description: Models refreshed daily in production
    definition:
      union:
        - tag:daily
        - config.materialized:incremental
      exclude:
        - tag:deprecated
  
  - name: critical_path
    description: Models for critical dashboards
    definition:
      union:
        - exposure:executive_dashboard
        - exposure:customer_churn_model
        - exposure:revenue_forecasting
  
  - name: slim_ci
    description: Modified models for CI/CD
    definition:
      union:
        - state:modified+
        - state:new
      exclude:
        - resource_type:snapshot

# Usage:
# dbt run --selector daily_refresh
# dbt build --selector critical_path
```

**SDLC Use Case: Scheduled Production Runs**
```bash
# Hourly: high-frequency incremental models
0 * * * * dbt run --select tag:hourly --target prod

# Daily 2am: full daily refresh
0 2 * * * dbt run --select tag:daily --target prod

# Weekly Sunday: full refresh models
0 3 * * 0 dbt run --select tag:weekly --full-refresh --target prod

# Monthly: snapshots (SCD Type 2)
0 4 1 * * dbt snapshot --select tag:monthly --target prod
```

---

## 9.3 Cost Optimization

### What is Cost Optimization?
Strategies to reduce compute costs: incremental materialization, clustering/partitioning, warehouse auto-scaling, query result caching.

**Diagram: Cost Reduction Strategies**
```text
Full Table Scan (100M rows, 10min, $5)
     â†“ apply clustering
Clustered Scan (5M rows, 30sec, $0.25) â†’ 95% cost reduction
     â†“ apply incremental strategy
Incremental Merge (1M new rows, 10sec, $0.05) â†’ 99% cost reduction
     â†“ apply result caching
Cached Query (0sec, $0.00) â†’ 100% cost reduction
```

### Strategy 1: Incremental Materialization

**SYNTAX TEMPLATE**
```sql
{{
  config(
    materialized='incremental',
    unique_key='<primary_key>',
    incremental_strategy='merge | delete+insert | append',
    on_schema_change='append_new_columns | fail | sync_all_columns | ignore',
    cluster_by=['<column1>', '<column2>'],
    partition_by={'field': '<date_column>', 'data_type': 'date'}
  )
}}

SELECT 
  primary_key,
  updated_at,
  ...
FROM {{ ref('source_model') }}

{% if is_incremental() %}
  WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})
{% endif %}
```

**PARAMETERS**

| Parameter | Type | Default | Description | Example |
|-----------|------|---------|-------------|---------|
| incremental_strategy | Enum | merge | How to handle updates | merge, delete+insert, append, insert_overwrite |
| unique_key | String/Array | NULL | Primary key for deduplication | order_id, ['customer_id', 'order_date'] |
| on_schema_change | Enum | ignore | Handle schema drift | append_new_columns, sync_all_columns, fail |
| cluster_by | Array | [] | Clustering keys (Snowflake/BigQuery) | ['order_date', 'region'] |
| partition_by | Object | NULL | Partitioning config (BigQuery) | {field: 'order_date', data_type: 'date'} |

**BASIC EXAMPLE: Append-Only Incremental**
```sql
-- models/marts/fct_events.sql
-- Cost: Process 1M new rows/day instead of 1B total rows
{{
  config(
    materialized='incremental',
    unique_key='event_id',
    incremental_strategy='append'
  )
}}

SELECT 
  event_id,
  event_timestamp,
  user_id,
  event_type
FROM {{ ref('stg_events') }}

{% if is_incremental() %}
  WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

**ADVANCED EXAMPLE: Merge Strategy with Late Arriving Data**
```sql
-- models/marts/fct_orders.sql
-- Handle updates to existing orders (refunds, cancellations)
{{
  config(
    materialized='incremental',
    unique_key='order_id',
    incremental_strategy='merge',
    cluster_by=['order_date', 'customer_region'],
    on_schema_change='sync_all_columns'
  )
}}

SELECT 
  order_id,
  order_date,
  customer_id,
  customer_region,
  order_status,
  order_amount,
  updated_at
FROM {{ ref('stg_orders') }}

{% if is_incremental() %}
  -- Process last 7 days to handle late updates
  WHERE updated_at >= DATEADD(day, -7, CURRENT_DATE())
{% endif %}
```

### Strategy 2: Warehouse Auto-Scaling

**Snowflake: Dynamic Warehouse Sizing**
```sql
-- macros/set_warehouse_size.sql
{% macro set_warehouse_size(size='SMALL') %}
  {% if target.name == 'prod' and execute %}
    {% set query %}
      ALTER WAREHOUSE {{ target.warehouse }} SET WAREHOUSE_SIZE = '{{ size }}'
    {% endset %}
    {% do run_query(query) %}
    {{ log("Warehouse set to " ~ size, info=True) }}
  {% endif %}
{% endmacro %}

-- Model with dynamic sizing:
-- models/marts/fct_daily_aggregates.sql
{% if is_incremental() %}
  {{ set_warehouse_size('MEDIUM') }}
{% else %}
  {{ set_warehouse_size('X-LARGE') }}  -- Full refresh needs more compute
{% endif %}

SELECT ...

{{ set_warehouse_size('SMALL') }}  -- Scale down after completion
```

**BigQuery: Slot Reservations**
```yaml
# dbt_project.yml
models:
  my_project:
    marts:
      +labels:
        priority: high
        cost_center: analytics
      
      # Use flex slots for batch jobs (lower cost)
      fct_historical_summary:
        +priority: batch  # vs interactive
```

### Strategy 3: Query Result Caching

**Leverage Warehouse Caching**
```sql
-- models/staging/stg_orders.sql
-- Run hourly, results cached for 24h in Snowflake
{{
  config(
    materialized='view'  -- View benefits from query result cache
  )
}}

SELECT 
  order_id,
  order_date,
  customer_id,
  -- Deterministic logic (same input â†’ same output)
  ROUND(amount / 100.0, 2) AS amount_dollars
FROM {{ source('raw', 'orders') }}
WHERE order_date >= CURRENT_DATE() - 7  -- Rolling 7-day window

-- Subsequent queries hit cache if:
-- 1. SQL text identical
-- 2. Source data unchanged
-- 3. Within cache TTL (24h Snowflake, 6h BigQuery)
```

### Strategy 4: Partition Pruning

**BigQuery: Date Partitioning**
```sql
-- models/marts/fct_events.sql
-- Cost: Scan 1 day of data instead of 3 years
{{
  config(
    materialized='table',
    partition_by={
      'field': 'event_date',
      'data_type': 'date',
      'granularity': 'day'
    },
    require_partition_filter=true,  -- Enforce WHERE clause on event_date
    cluster_by=['event_type', 'user_id']
  )
}}

SELECT 
  event_id,
  event_date,
  event_type,
  user_id
FROM {{ ref('stg_events') }}

-- Queries MUST include: WHERE event_date = '2025-01-31'
-- Cost reduction: 99.7% (1 day / 3 years)
```

**Snowflake: Clustering for Pruning**
```sql
-- models/marts/fct_transactions.sql
{{
  config(
    materialized='table',
    cluster_by=['transaction_date::date', 'merchant_category'],
    automatic_clustering=true
  )
}}

SELECT ...

-- Snowflake micro-partition pruning:
-- WHERE transaction_date = '2025-01-31'
-- Scans 0.1% of partitions instead of 100%
```

### Cost Monitoring

**Query Tags for Cost Attribution**
```yaml
# dbt_project.yml
models:
  my_project:
    +query_tag: "dbt:{{ model.name }}:{{ target.name }}:{{ invocation_id }}"

# Snowflake query history:
SELECT 
  query_tag,
  SUM(credits_used_cloud_services) AS total_credits,
  COUNT(*) AS query_count
FROM snowflake.account_usage.query_history
WHERE query_tag LIKE 'dbt:%'
  AND start_time >= DATEADD(day, -7, CURRENT_TIMESTAMP())
GROUP BY 1
ORDER BY 2 DESC;
```

**SDLC Use Case: Cost Budget Enforcement**
```python
# scripts/cost_gate_check.py
import snowflake.connector
import sys

conn = snowflake.connector.connect(...)
cursor = conn.cursor()

# Check last 24h dbt costs
cursor.execute("""
  SELECT SUM(credits_used_cloud_services) AS daily_cost
  FROM snowflake.account_usage.query_history
  WHERE query_tag LIKE 'dbt:%'
    AND start_time >= DATEADD(hour, -24, CURRENT_TIMESTAMP())
""")

daily_cost = cursor.fetchone()[0] or 0

BUDGET_LIMIT = 50  # $50/day budget
if daily_cost > BUDGET_LIMIT:
    print(f"COST GATE FAILED: ${daily_cost:.2f} exceeds ${BUDGET_LIMIT} budget")
    sys.exit(1)
else:
    print(f"Cost check passed: ${daily_cost:.2f}")
```

---

## 9.4 Logging & Monitoring

### What is Logging?
dbt logs execution events (start/end times, row counts, errors) to stdout and run_results.json. Production deployments should centralize logs for alerting and analytics.

**Diagram: Logging Flow**
```text
dbt run (execution)
     â†“ stdout
dbt Logs (console output)
     â†“ log_path (logs/dbt.log)
Local Log File
     â†“ CI/CD (GitHub Actions, Airflow)
CloudWatch / Datadog / Splunk
     â†“ alerts
PagerDuty / Slack (on failure)
```

### Syntax: Logging Configuration

**dbt_project.yml**
```yaml
name: my_project
version: '1.0.0'

# Logging configuration
log-path: "logs"
target-path: "target"

# Log level (debug, info, warn, error)
# Set via CLI: dbt run --log-level debug
```

**CLI Logging Flags**
```bash
# Verbose logging (debug mode)
dbt run --log-level debug

# Show compiled SQL
dbt run --debug

# Print logs to file
dbt run --log-path logs/ > logs/dbt_run_$(date +%Y%m%d_%H%M%S).log 2>&1

# JSON-formatted logs (for structured parsing)
dbt run --log-format json
```

### Structured Logging Example

**Parse run_results.json in CI/CD**
```python
# scripts/parse_run_results.py
import json
from datetime import datetime

with open('target/run_results.json') as f:
    results = json.load(f)

failed_models = []
for result in results['results']:
    if result['status'] in ['error', 'fail']:
        failed_models.append({
            'model': result['unique_id'],
            'status': result['status'],
            'message': result.get('message', 'N/A'),
            'execution_time': result['execution_time']
        })

if failed_models:
    print("âŒ FAILED MODELS:")
    for model in failed_models:
        print(f"  - {model['model']}: {model['status']} ({model['execution_time']:.2f}s)")
        print(f"    Error: {model['message']}")
    exit(1)
else:
    print("âœ… All models succeeded")
```

### Custom Logging Macro

```sql
-- macros/log_model_metrics.sql
{% macro log_model_metrics() %}
  {% if execute %}
    {% set model_name = this.identifier %}
    {% set row_count_query %}
      SELECT COUNT(*) AS row_count FROM {{ this }}
    {% endset %}
    
    {% set results = run_query(row_count_query) %}
    {% if results %}
      {% set row_count = results.columns[0][0] %}
      {{ log("Model " ~ model_name ~ " row count: " ~ row_count, info=True) }}
    {% endif %}
  {% endif %}
{% endmacro %}

-- Usage in model:
-- models/marts/fct_orders.sql
SELECT ...
FROM {{ ref('stg_orders') }}

{{ log_model_metrics() }}  -- Post-hook
```

### Monitoring Dashboards

**Snowflake: dbt Query Performance**
```sql
-- Create monitoring view
CREATE OR REPLACE VIEW analytics.dbt_monitoring.query_performance AS
SELECT 
  SPLIT_PART(query_tag, ':', 2) AS model_name,
  SPLIT_PART(query_tag, ':', 3) AS environment,
  COUNT(*) AS execution_count,
  AVG(execution_time / 1000) AS avg_execution_seconds,
  MAX(execution_time / 1000) AS max_execution_seconds,
  SUM(credits_used_cloud_services) AS total_credits,
  MAX(start_time) AS last_run_time
FROM snowflake.account_usage.query_history
WHERE query_tag LIKE 'dbt:%'
  AND start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP())
GROUP BY 1, 2
ORDER BY total_credits DESC;

-- Alert on slow models (>5 min)
SELECT 
  model_name,
  avg_execution_seconds
FROM analytics.dbt_monitoring.query_performance
WHERE avg_execution_seconds > 300
ORDER BY avg_execution_seconds DESC;
```

**BigQuery: dbt Cost Tracking**
```sql
-- BigQuery audit logs via Log Explorer
SELECT 
  REGEXP_EXTRACT(labels.value, r'dbt:([^:]+)') AS model_name,
  SUM(total_bytes_billed) / POW(10, 12) AS tb_billed,
  SUM(total_slot_ms) / 1000 / 60 AS slot_minutes,
  COUNT(*) AS query_count
FROM `project.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE labels.key = 'query_tag'
  AND labels.value LIKE 'dbt:%'
  AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY 1
ORDER BY 2 DESC;
```

### Alerting Integration

**Slack Alerts via Webhook**
```bash
# scripts/notify_slack_on_failure.sh
#!/bin/bash

if [ "$DBT_RUN_STATUS" == "failed" ]; then
  curl -X POST $SLACK_WEBHOOK_URL \
    -H 'Content-Type: application/json' \
    -d '{
      "text": "ðŸš¨ dbt Production Run Failed",
      "blocks": [
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "*dbt run failed in production*\n\nEnvironment: `'$TARGET_ENV'`\nTimestamp: `'$(date)'`\nRun ID: `'$INVOCATION_ID'`"
          }
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "Check logs: <https://github.com/org/repo/actions/runs/'$GITHUB_RUN_ID'|GitHub Actions>"
          }
        }
      ]
    }'
fi
```

**PagerDuty Integration**
```python
# scripts/trigger_pagerduty_incident.py
import requests
import sys

PAGERDUTY_ROUTING_KEY = 'your_routing_key'

def trigger_incident(severity, summary, details):
    payload = {
        'routing_key': PAGERDUTY_ROUTING_KEY,
        'event_action': 'trigger',
        'payload': {
            'summary': summary,
            'severity': severity,  # critical, error, warning, info
            'source': 'dbt-production',
            'custom_details': details
        }
    }
    
    response = requests.post(
        'https://events.pagerduty.com/v2/enqueue',
        json=payload
    )
    
    if response.status_code == 202:
        print(f"âœ… PagerDuty incident triggered: {response.json()['dedup_key']}")
    else:
        print(f"âŒ Failed to trigger incident: {response.text}")
        sys.exit(1)

# Usage:
if __name__ == '__main__':
    trigger_incident(
        severity='critical',
        summary='dbt production run failed',
        details={
            'failed_models': ['fct_orders', 'dim_customers'],
            'environment': 'production',
            'run_id': 'abc123'
        }
    )
```

---

## 9.5 Production Release Process

### SDLC: Blue/Green Deployment

**Strategy**: Build new models in "blue" schema, test, then swap to production.

**Diagram: Blue/Green Release Flow**
```text
Production Schema (DBT_PROD) â†’ Active
     â†“ clone
Blue Schema (DBT_BLUE) â†’ Build + Test
     â†“ validation passed
Swap Schemas (PROD â†” BLUE)
     â†“ rollback if issues
Green Schema (DBT_GREEN) â†’ Previous version (instant rollback)
```

**Implementation**
```bash
# 1. Build in blue environment
dbt build --target blue --full-refresh

# 2. Run data quality tests
dbt test --target blue

# 3. Smoke test critical queries
dbt run-operation validate_critical_metrics --args '{schema: blue}'

# 4. Swap schemas (atomic operation)
snowsql -q "
  ALTER SCHEMA ANALYTICS_DB.DBT_PROD RENAME TO DBT_GREEN;
  ALTER SCHEMA ANALYTICS_DB.DBT_BLUE RENAME TO DBT_PROD;
  ALTER SCHEMA ANALYTICS_DB.DBT_GREEN RENAME TO DBT_BLUE;
"

# 5. Monitor for 1 hour, rollback if needed
sleep 3600
if [ $(check_error_rate) -gt 0 ]; then
  snowsql -q "
    ALTER SCHEMA ANALYTICS_DB.DBT_PROD RENAME TO DBT_BLUE;
    ALTER SCHEMA ANALYTICS_DB.DBT_GREEN RENAME TO DBT_PROD;
  "
  echo "Rollback completed"
fi
```

### SDLC: Canary Deployment

**Strategy**: Deploy to subset of users/regions first, monitor, then full rollout.

```bash
# 1. Deploy to canary schema (10% of data)
dbt run --target canary --vars '{region_filter: "US-WEST"}'

# 2. Run parallel queries (prod vs canary)
dbt run-operation compare_results --args '{
  prod_schema: dbt_prod,
  canary_schema: dbt_canary,
  tolerance: 0.01
}'

# 3. If metrics match, promote to prod
if [ $? -eq 0 ]; then
  dbt run --target prod --full-refresh
else
  echo "Canary validation failed - aborting deployment"
  exit 1
fi
```

### SDLC: GitHub Release Workflow

**Complete CI/CD Pipeline**
```yaml
# .github/workflows/dbt_production_release.yml
name: dbt Production Release

on:
  push:
    tags:
      - 'v*'  # Trigger on version tags (v1.0.0, v1.1.0)

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production  # GitHub environment protection rules
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dbt
        run: pip install dbt-snowflake==1.7.0
      
      - name: Download production artifacts
        run: |
          mkdir -p prod_artifacts
          aws s3 sync s3://dbt-artifacts/prod/ prod_artifacts/
      
      - name: dbt deps
        run: dbt deps
      
      - name: dbt compile (validation)
        run: dbt compile --target prod
      
      - name: Run modified models (slim deployment)
        run: |
          dbt build \
            --select state:modified+ \
            --state prod_artifacts \
            --defer \
            --target prod
        env:
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PROD_PASSWORD }}
      
      - name: Run full test suite
        run: dbt test --target prod
      
      - name: Generate documentation
        run: dbt docs generate --target prod
      
      - name: Upload artifacts to S3
        run: |
          aws s3 sync target/ s3://dbt-artifacts/prod/
          aws s3 cp target/manifest.json s3://dbt-artifacts/versioned/manifest_${{ github.ref_name }}.json
      
      - name: Deploy docs to S3
        run: aws s3 sync target/ s3://dbt-docs-prod/ --exclude "*.json"
      
      - name: Create GitHub Release
        uses: actions/create-release@v1
        with:
          tag_name: ${{ github.ref }}
          release_name: Release ${{ github.ref }}
          body: |
            ## dbt Production Deployment
            - Models updated: [See run results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - Documentation: https://dbt-docs.company.com
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Notify Slack (success)
        if: success()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
            -d '{"text":"âœ… dbt production deployed successfully: ${{ github.ref }}"}'
      
      - name: Notify PagerDuty (failure)
        if: failure()
        run: python scripts/trigger_pagerduty_incident.py
```

---

## 9.6 Performance Optimization Checklist

**Pre-Deployment Checklist**
```markdown
## Performance Review

### Model Optimization
- [ ] Incremental models use `unique_key` and appropriate `incremental_strategy`
- [ ] Large tables (>10M rows) use clustering/partitioning
- [ ] Views replaced with tables for frequently queried models
- [ ] Ephemeral models used for simple transformations (avoid intermediate tables)

### Query Optimization
- [ ] SELECT only required columns (avoid SELECT *)
- [ ] Filters applied early in CTEs
- [ ] Avoid DISTINCT when unnecessary (use GROUP BY or window functions)
- [ ] Use QUALIFY for window function filtering (Snowflake/BigQuery)

### Cost Control
- [ ] Query tags configured for cost attribution
- [ ] Warehouse auto-scaling implemented
- [ ] Partition pruning enforced (require_partition_filter=true)
- [ ] Transient tables used for staging/intermediate models

### Testing & Monitoring
- [ ] Generic tests added for all primary keys
- [ ] Freshness checks on critical sources
- [ ] Row count monitoring for critical models
- [ ] Alerts configured for test failures
- [ ] Cost budget gates in CI/CD
```

---

## Summary

**State Management**: Use artifacts (manifest.json, run_results.json) for slim CI, deployment validation, and lineage tracking.

**Selective Runs**: Master --select/--exclude flags and state: selectors to run only modified models (95% faster CI/CD).

**Cost Optimization**: Incremental materialization, clustering/partitioning, warehouse auto-scaling, and query result caching reduce costs by 90%+.

**Logging & Monitoring**: Centralize logs (CloudWatch, Datadog), track query performance (Snowflake ACCOUNT_USAGE), and alert on failures (Slack, PagerDuty).

**Production Release**: Blue/green deployments, canary testing, automated GitHub releases with artifact versioning.

---
